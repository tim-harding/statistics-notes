\documentclass{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage[margin=0.5in]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\newcommand{\var}{\text{Var}}
\newcommand{\bernoulli}{\text{Bernoulli}}
\newcommand{\binomial}{\text{Binomial}}
\newcommand{\poisson}{\text{Poisson}}
\newcommand{\stdev}{\text{Stdev}}

\title{Probability and Statistics Notes}
\author{Tim Harding}
\date{Autumn 2021}

\begin{document}
\begin{multicols*}{2}

\section{Probability}

\subsection{Properties of Probability}

\textbf{Random experiments}: Experiments where the outcome is uncertain.

\textbf{Outcome space}: The set of all possible outcomes $S$.

\textbf{Event}: A set of outcomes $A$ such that $A \in S$. The event $A$ has \textit{occurred} when some outcome of a random experiment $a$ occurs where $a \in A$.

\textbf{Mutually exclusive events}: $A$ and $B$ are mutually exclusive if $A \cap B = \emptyset$

\textbf{Exhaustive events}: The set $A_1, A_2, \ldots, A_k$ is exhaustive if $A_1 \cup A_2 \cup \ldots \cup A_k = S$

\textbf{Commutativity and Associativity}: Sets are commutative and associative under the same operator. Commutivity and associativity do not hold under a mix of $\cup$ and $\cap$.
\begin{align*}
    A \cup B &= B \cup A \\
    (A \cup B) \cup C &= A \cup (B \cup C)
\end{align*}

\textbf{Distributive laws}:
\begin{enumerate}
    \item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
    \item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\end{enumerate}

\textbf{DeMorgan's laws}:
\begin{enumerate}
    \item $(A \cap B)' = A' \cup B'$
    \item $(A \cup B)' = A' \cap B'$
\end{enumerate}

\textbf{Some primitive theorems}:
These are given as 1.1-1 through 1.1-4
\begin{enumerate}
    \item $P(A) = 1 - P(A')$
    \item $P(\emptyset) = 0$
    \item $A \subset B \implies P(A) \leq P(B)$
    \item $P(A) \leq 1$
\end{enumerate}

\textbf{Frequency}: $N(A)$ is the number of times $A$ occurred in $n$ repetitions.

\textbf{Relative frequency}:
\begin{align*}
    \frac{N(A)}{n}
\end{align*}

\textbf{Probability}: $P(A)$ is the relative frequency of $A$ as $n$ grows arbitrarily large. It satisfies the following properties for the mutually exclusive events $A_1, A_2, \ldots, A_k$ where $k$ may be infinite:
\begin{enumerate}
    \item $P(A_n) \geq 0$
    \item $P(S) = 1$
    \item $P(A_1 \cup A_2 \cup \ldots \cup A_k) = \sum_{n=1}^k{P(A_n)}$
\end{enumerate}

Also note that
\[P(A) = \frac{N(A)}{N(S)}\]
when discussing permutations and combinations.

\textbf{Additive law}: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

\textbf{Mutual exclusivity}:
\begin{align*}
    P(A \cap B) = \emptyset
\end{align*}

\subsection{Methods of Enumeration}

\textbf{Multiplication Principle}: Given experiments $E_1$ with $n_1$ possible outcomes and $E_2$ with $n_2$ possible outcomes, the composite experiment $E_1 E_2$ has $n_1 n_2$ possible outcomes.

\textbf{Permutation}: An arrangement of $n$ objects. $n!$ arrangements are possible.

\textbf{Ordered sample}: An ordered sample of $r$ objects taken from a set of $n$ objects.

\textbf{Sampling with replacement}: When a given object may be selected multiple times in a sample. There are $n^r$ ordered samples with replacement.

\textbf{Sampling without replacement}: Each object may be selected only once in a sample.

\textbf{Permutations of $\mathbf{n}$ taken $\mathbf{r}$}:
The number of permutations for $n$ objects filling $r$ positions.
\[{}_n P_r = \frac{n!}{(n-r)!}\]

There are ${}_n P_r$ ordered samples without replacement.

\textbf{Combinations of $\mathbf{n}$ taken $\mathbf{r}$}: The number of unordered subsets of size $r$ that can be selected from $n$ objects where $r \leq n$. This is pronounced \textit{n choose r}.
\[{}_n C_r = \begin{pmatrix}
    n \\
    r
\end{pmatrix} = \frac{n!}{r! (n-r)!} = \frac{{}_n P_r}{r!}\]

There are ${}_n C_r$ unordered samples without replacement.

\textbf{Binomial coefficients}: The numbers $n$ and $r$.

\textbf{Distinguishable permutation}: One of the ${}_n C_r$ permutations of $n$ objects.

\subsection{Conditional Probability}

\textbf{High-level notes}:
\begin{enumerate}
    \item Tables are useful for evaluating conditional probabilities.
\end{enumerate}

\textbf{Conditional Probability Formula}:
\begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation*}

\textbf{Multiplicative rule}:
This is just a rearrangement of the conditional probability formula.
\begin{equation*}
    P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)
\end{equation*}

\subsection{Independence}

\textbf{Independence test}: $P(A \cap B) = P(A) P(B)$

If $A$ and $B$ are independent, then so are the following:
\begin{enumerate}
    \item $A'$ and $B$
    \item $A$ and $B'$
    \item $A'$ and $B'$
\end{enumerate}

\subsection{Bayes' Theorem}

\textbf{Law of Total Probability}: If the events $A_1, \ldots, A_k$ are mutually exclusive and exhaustive, then for any other event $B$,
\begin{equation*}
    B = (B \cap A_1) \cup \ldots \cup (B \cap A_k)
\end{equation*}
Therefore,
\begin{align*}
    P(B) &= \sum_{i = 1}^k P(B \cap A_1) \\
    &= \sum_{i = 1}^k P(B|A_i) P(A_i)
\end{align*}

\textbf{Bayes' Theorem}:
Allows us to find $P(A_j|B)$ if we have information about $P(B|A_j)$. This derivation uses the conditional probability theorem followed by the multiplicative rule and the law of total probability.
\begin{align*}
    P(A_j|B) &= \frac{P(A_j \cap B)}{P(B)} \\
    &= \frac{P(B|A_j) P(A_j)}{\sum_{i = 1}^k P(B|A_i) P(A_i)}
\end{align*}

\section{Discrete Distributions}

\subsection{Discrete random variables}

\subsubsection{Probability Mass Function}
$f(x) = p(x) = f(X = x)$ gives the probability of each value $x$ that a random variable $X$ can take on.

\subsubsection{Cumulative Distribution Function}
\begin{align*}
    F(x) = f(X \leq x)
\end{align*}

\subsection{Mathematical expectation}
\begin{align*}
    E(X) &= \sum_{x \in D} x \times f(x) \\
    E(u(X)) &= \sum_{x \in D} u(x) \times f(x) \\
    E(c_1 u_1(X) + c_2 u_2(X)) &= c_1 E(u_1(X)) + c_2 E(u_2(X))
\end{align*}

\subsection{Special mathematical expectations}
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & Variance & Standard deviation \\ \hline
        Population & $\sigma^2$ & $\sigma$ \\ \hline
        Sample & $s^2$ & $s$ \\ \hline
    \end{tabular}
\end{center}

\begin{align*}
    \sigma^2 &= E((X - \mu)^2) \\
    &= \sum_{x \in D} (x - \mu)^2 \times f(x)
\end{align*}
\begin{align*}
    s^2 &= \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \bar{x})^2 \\
    &= \frac{1}{n - 1} \sum_{i = 1}^n (x_i^2 - n\bar{x}^2)
\end{align*}
\begin{align*}
    \var(cX) &= c^2 \var(X) \\
    \var(X + c) &= \var(X)
\end{align*}
\begin{align*}
    \stdev(X) &= \sqrt{\var(X)}
\end{align*}

\subsection{Bernoulli distribution}

Used for experiments where there are two outcomes. $p \equiv$ probability of success for each experiment.
\begin{align*}
    X &\sim \bernoulli(p) \\
    f(x) &= p^x (1-p)^{1-x} \\
    E(X) &= p \\
    \var(X) &= p(1-p)
\end{align*}

\subsection{Binomial distribution}
$n \equiv$ number of trials, $p \equiv$ probability of success for each trial.
\begin{align*}
    X &\sim \binomial(n, p) \\
    f(x) &= \begin{pmatrix}
        n \\
        x
    \end{pmatrix} p^x (1-p)^{n-x} \\
    E(X) &= np \\
    \var(X) &= np(1-p)
\end{align*}

\subsection{Poisson distribution}
Used when you expect to see $\lambda$ occurances over some interval. Also used to approximate a binomial distribution when $n$ is large and $p$ is small.
\begin{align*}
    X &\sim \poisson(\lambda) \\
    f(x) &= \frac{e^{-\lambda} \lambda^x}{x!} \\
    E(X) &= \lambda \\
    \var(X) &= \lambda
\end{align*}

\end{multicols*}
\end{document}