\documentclass{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage[margin=0.5in]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\newcommand{\var}{\text{Var}}
\newcommand{\bernoulli}{\text{Bernoulli}}
\newcommand{\binomial}{\text{Binomial}}
\newcommand{\poisson}{\text{Poisson}}
\newcommand{\stdev}{\text{Stdev}}

\title{Probability and Statistics Notes}
\author{Tim Harding}
\date{Autumn 2021}

\begin{document}
\begin{multicols*}{2}

\section{Probability}

\subsection{Properties of Probability}

\textbf{Mutual exclusivity}: $A \cap B = \emptyset$

\textbf{Exhaustive events}: $A_1 \cup A_2 \cup \ldots \cup A_k = S$

\textbf{Commutativity and Associativity}:
\begin{align*}
    A \cup B &= B \cup A \\
    (A \cup B) \cup C &= A \cup (B \cup C)
\end{align*}

\textbf{Distributive laws}:
\begin{enumerate}
    \item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
    \item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\end{enumerate}

\textbf{DeMorgan's laws}:
\begin{enumerate}
    \item $(A \cap B)' = A' \cup B'$
    \item $(A \cup B)' = A' \cap B'$
\end{enumerate}

\textbf{Primitive theorems}:
\begin{enumerate}
    \item $P(A) = 1 - P(A')$
    \item $P(\emptyset) = 0$
    \item $A \subset B \implies P(A) \leq P(B)$
    \item $P(A) \leq 1$
\end{enumerate}

\textbf{Probability}:
\begin{enumerate}
    \item $P(A_n) \geq 0$
    \item $P(S) = 1$
    \item $P(A_1 \cup A_2 \cup \ldots \cup A_k) = \sum_{n=1}^k{P(A_n)}$
\end{enumerate}

\textbf{Additive law}: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

\textbf{Mutual exclusivity}: $P(A \cap B) = \emptyset$

\subsection{Methods of Enumeration}

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        Replacement & Ordered    & Unordered  \\ \hline
        With        & $n^r$      & $n!$       \\ \hline
        Without     & ${}_n P_r$ & ${}_n C_r$ \\ \hline
    \end{tabular}
\end{center}
\begin{align*}
    {}_n P_r &= \frac{n!}{(n-r)!} \\
    {}_n C_r &= \begin{pmatrix}
        n \\
        r
    \end{pmatrix} = \frac{{}_n P_r}{r!}
\end{align*}

\subsection{Conditional Probability}
Use tables for evaluating conditional probabilities.
\begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation*}

\subsection{Independence}

% TODO: There was another independence thing on the exam I wanted to write down
\textbf{Independence test}: $P(A \cap B) = P(A) P(B)$

\subsection{Bayes' Theorem}

\textbf{Law of Total Probability}: Given the mutually exclusive and exhaustive events $A_1, \ldots, A_k$
\begin{align*}
    P(B) &= \sum_{i = 1}^k P(B|A_i) P(A_i)
\end{align*}

\textbf{Bayes' Theorem}:
\begin{align*}
    P(A_j|B) &= \frac{P(B|A_j) P(A_j)}{\sum_{i = 1}^k P(B|A_i) P(A_i)}
\end{align*}

\section{Discrete Distributions}

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & Variance & Standard deviation \\ \hline
        Population & $\sigma^2$ & $\sigma$ \\ \hline
        Sample & $s^2$ & $s$ \\ \hline
    \end{tabular}
\end{center}
\begin{align*}
    E(X) &= \sum_{x \in D} x \times f(x) \\
    E(u(X)) &= \sum_{x \in D} u(x) \times f(x) \\
    E(c_1 u_1(X) + c_2 u_2(X)) &= c_1 E(u_1(X)) + c_2 E(u_2(X))
\end{align*}
\begin{align*}
    \var(X) &= E(X^2) - \mu^2 \\
    \var(cX) &= c^2 \var(X) \\
    \var(X + c) &= \var(X)
\end{align*}
\begin{align*}
    s^2 &= \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \bar{x})^2
\end{align*}

\subsection{Bernoulli distribution}

$p \equiv$ probability of success for each experiment.
\begin{align*}
    X &\sim \bernoulli(p) \\
    f(x) &= p^x (1-p)^{1-x} \\
    E(X) &= p \\
    \var(X) &= p(1-p)
\end{align*}

\subsection{Binomial distribution}
$n \equiv$ number of trials, $p \equiv$ probability of success for each trial.
\begin{align*}
    X &\sim \binomial(n, p) \\
    f(x) &= \begin{pmatrix}
        n \\
        x
    \end{pmatrix} p^x (1-p)^{n-x} \\
    E(X) &= np \\
    \var(X) &= np(1-p)
\end{align*}

\subsection{Poisson distribution}
Used when you expect to see $\lambda$ occurances over some interval. Also used to approximate a binomial distribution when $n$ is large and $p$ is small.
\begin{align*}
    X &\sim \poisson(\lambda) \\
    f(x) &= \frac{e^{-\lambda} \lambda^x}{x!} \\
    E(X) &= \lambda \\
    \var(X) &= \lambda
\end{align*}

\end{multicols*}
\end{document}