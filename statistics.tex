\documentclass{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage[margin=1in]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\title{Probability and Statistics Notes}
\author{Tim Harding}
\date{Autumn 2021}

\begin{document}
\begin{multicols*}{2}

\section{Probability}

\subsection{Properties of Probability}

\subsubsection{Random experiments}
Experiments where the outcome is uncertain.

\subsubsection{Outcome space}
The set of all possible outcomes $S$.

\subsubsection{Event}
A set of outcomes $A$ such that $A \in S$. The event $A$ has \textit{occurred} when some outcome of a random experiment $a$ occurs where $a \in A$.

\subsubsection{Mutually exclusive events}
$A$ and $B$ are mutually exclusive if $A \cap B = \emptyset$

\subsubsection{Exhaustive events}
The set $A_1, A_2, \ldots, A_k$ is exhaustive if $A_1 \cup A_2 \cup \ldots \cup A_k = S$

\subsubsection{Commutativity and Associativity}
Sets are commutative and associative.

\subsubsection{Distributive laws}
\begin{enumerate}
    \item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
    \item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\end{enumerate}

\subsubsection{DeMorgan's laws}
\begin{enumerate}
    \item $(A \cap B)' = A' \cup B'$
    \item $(A \cup B)' = A' \cap B'$
\end{enumerate}

\subsubsection{Some primitive theorems}
These are given as 1.1-1 through 1.1-4
\begin{enumerate}
    \item $P(A) = 1 - P(A')$
    \item $P(\emptyset) = 0$
    \item $A \subset B \implies P(A) \leq P(B)$
    \item $P(A) \leq 1$
\end{enumerate}

\subsubsection{Frequency}
$N(A)$ is the number of times $A$ occurred in $n$ repetitions.

\subsubsection{Relative frequency}
$\frac{N(A)}{n}$

\subsubsection{Probability}
$P(A)$ is the relative frequency of $A$ as $n$ grows arbitrarily large. It satisfies the following properties for the mutually exclusive events $A_1, A_2, \ldots, A_k$ where $k$ may be infinite:
\begin{enumerate}
    \item $P(A_n) \geq 0$
    \item $P(S) = 1$
    \item $P(A_1 \cup A_2 \cup \ldots \cup A_k) = \sum_{n=1}^k{P(A_n)}$
\end{enumerate}

Also note that
\[P(A) = \frac{N(A)}{N(S)}\]
when discussing permutations and combinations.

\subsubsection{Additive law}
$P(A \cup B) = P(A) + P(B) - P(A \cap B)$

\subsection{Methods of Enumeration}

\subsubsection{Multiplication Principle}
Given experiments $E_1$ with $n_1$ possible outcomes and $E_2$ with $n_2$ possible outcomes, the composite experiment $E_1 E_2$ has $n_1 n_2$ possible outcomes.

\subsubsection{Permutation}
An arrangement of $n$ objects. $n!$ arrangements are possible.

\subsubsection{Ordered sample}
An ordered sample of $r$ objects taken from a set of $n$ objects.

\subsubsection{Sampling with replacement}
When a given object may be selected multiple times in a sample. There are $n^r$ ordered samples with replacement.

\subsubsection{Sampling without replacement}
Each object may be selected only once in a sample.

\subsubsection{Permutations of $\mathbf{n}$ taken $\mathbf{r}$}
The number of permutations for $n$ objects filling $r$ positions.
\[{}_n P_r = \frac{n!}{(n-r)!}\]

There are ${}_n P_r$ ordered samples without replacement.

\subsubsection{Combinations of $\mathbf{n}$ taken $\mathbf{r}$}
The number of unordered subsets of size $r$ that can be selected from $n$ objects where $r \leq n$. This is pronounced \textit{n choose r}.
\[{}_n C_r = \begin{pmatrix}
    n \\
    r
\end{pmatrix} = \frac{n!}{r! (n-r)!} = \frac{{}_n P_r}{r!}\]

There are ${}_n C_r$ unordered samples without replacement.

\subsubsection{Binomial coefficients}
The numbers $n$ and $r$.

\subsubsection{Distinguishable permutation}
One of the ${}_n C_r$ permutations of $n$ objects.

\subsection{Conditional Probability}

\subsubsection{High-level notes}
\begin{enumerate}
    \item Tables are useful for evaluating conditional probabilities.
\end{enumerate}

\subsubsection{Conditional Probability Formula}
\begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation*}

\subsubsection{Multiplicative rule}
This is just a rearrangement of the conditional probability formula.
\begin{equation*}
    P(A \cap B) = P(A|B) P(B) = P(B|A) P(A)
\end{equation*}

\subsection{Independence}

\subsubsection{Independence test}
$P(A \cap B) = P(A) P(B)$

If $A$ and $B$ are independent, then so are the following:
\begin{enumerate}
    \item $A'$ and $B$
    \item $A$ and $B'$
    \item $A'$ and $B'$
\end{enumerate}

\subsection{Bayes' Theorem}

\subsubsection{Law of Total Probability}
If the events $A_1, \ldots, A_k$ are mutually exclusive and exhaustive, then for any other event $B$,
\begin{equation*}
    B = (B \cap A_1) \cup \ldots \cup (B \cap A_k)
\end{equation*}
Therefore,
\begin{align*}
    P(B) &= \sum_{i = 1}^k P(B \cap A_1) \\
    &= \sum_{i = 1}^k P(B|A_i) P(A_i)
\end{align*}

\subsubsection{Bayes' Theorem}
Allows us to find $P(A_j|B)$ if we have information about $P(B|A_j)$. This derivation uses the conditional probability theorem followed by the multiplicative rule and the law of total probability.
\begin{align*}
    P(A_j|B) &= \frac{P(A_j \cap B)}{P(B)} \\
    &= \frac{P(B|A_j) P(A_j)}{\sum_{i = 1}^k P(B|A_i) P(A_i)}
\end{align*}

\section{Discrete Distributions}

\subsection{Discrete random variables}

\subsubsection{Probability Mass Function}
$f(x)$ gives the probability of each value $x$ that a random variable $X$ can take on.

\subsubsection{Cumulative Distribution Function}
\begin{align*}
    F(x) = f(X \leq x)
\end{align*}

\subsection{Mathematical Expectation}

\subsubsection{Mean}
\begin{align*}
    E(X) = \mu_x = \sum x \times p(x)
\end{align*}

\end{multicols*}
\end{document}