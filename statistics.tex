\documentclass{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage[margin=0.45in]{geometry}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\newcommand{\var}{\text{Var}}
\newcommand{\bernoulli}{\text{Bernoulli}}
\newcommand{\binomial}{\text{Binomial}}
\newcommand{\poisson}{\text{Poisson}}
\newcommand{\stdev}{\text{Stdev}}
\newcommand{\exponential}{\text{Exponential}}
\newcommand{\normal}{\text{Normal}}
\newcommand{\normalcdf}{\text{Normalcdf}}
\newcommand{\tcdf}{\text{Tcdf}}
\newcommand{\curly}[1]{\left\{ #1 \right\}}
\newcommand{\soft}[1]{\left( #1 \right)}
\newcommand{\hard}[1]{\left[ #1 \right]}

\pagenumbering{gobble}

\title{Probability and Statistics Notes}
\author{Tim Harding}
\date{Autumn 2021}

\begin{document}
\begin{multicols*}{2}

\section{Probability}

\subsection{Properties of Probability}

\textbf{Mutual exclusivity}: $A \cap B = \emptyset$

\textbf{Exhaustive events}: $A_1 \cup A_2 \cup \ldots \cup A_k = S$

\textbf{Commutativity and Associativity}:
\begin{align*}
    A \cup B &= B \cup A \\
    (A \cup B) \cup C &= A \cup (B \cup C)
\end{align*}

\textbf{Distributive laws}:
\begin{enumerate}
    \item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
    \item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\end{enumerate}

\textbf{DeMorgan's laws}:
\begin{enumerate}
    \item $(A \cap B)' = A' \cup B'$
    \item $(A \cup B)' = A' \cap B'$
\end{enumerate}

\textbf{Primitive theorems}:
\begin{enumerate}
    \item $P(A) = 1 - P(A')$
    \item $P(\emptyset) = 0$
    \item $A \subset B \implies P(A) \leq P(B)$
    \item $P(A) \leq 1$
\end{enumerate}

\textbf{Probability}:
\begin{enumerate}
    \item $P(A_n) \geq 0$
    \item $P(S) = 1$
    \item $P(A_1 \cup A_2 \cup \ldots \cup A_k) = \sum_{n=1}^k{P(A_n)}$
\end{enumerate}

\textbf{Additive law}: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

\textbf{Mutual exclusivity}: $P(A \cap B) = \emptyset$

\subsection{Methods of Enumeration}

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        Replacement & Ordered    & Unordered  \\ \hline
        With        & $n^r$      & $n!$       \\ \hline
        Without     & ${}_n P_r$ & ${}_n C_r$ \\ \hline
    \end{tabular}
\end{center}
\begin{align*}
    {}_n P_r &= \frac{n!}{(n-r)!} \\
    {}_n C_r &= \begin{pmatrix}
        n \\
        r
    \end{pmatrix} = \frac{{}_n P_r}{r!}
\end{align*}

\subsection{Conditional Probability}
Use tables and tree diagrams for evaluating conditional probabilities.
\begin{equation*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{equation*}

\subsection{Independence}

% TODO: There was another independence thing on the exam I wanted to write down
\textbf{Independence test}: $P(A \cap B) = P(A) P(B)$

\subsection{Bayes' Theorem}

\textbf{Law of Total Probability}: Given the mutually exclusive and exhaustive events $A_1, \ldots, A_k$
\begin{align*}
    P(B) &= \sum_{i = 1}^k P(B|A_i) P(A_i)
\end{align*}

\textbf{Bayes' Theorem}:
\begin{align*}
    P(A_j|B) &= \frac{P(B|A_j) P(A_j)}{\sum_{i = 1}^k P(B|A_i) P(A_i)}
\end{align*}

\section{Discrete Distributions}

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
                   & Variance   & Standard deviation \\ \hline
        Population & $\sigma^2$ & $\sigma$           \\ \hline
        Sample     & $s^2$      & $s$                \\ \hline
    \end{tabular}
\end{center}
\begin{align*}
    E(X) &= \sum_{x \in D} x \times f(x) \\
    E(u(X)) &= \sum_{x \in D} u(x) \times f(x) \\
    E(c_1 u_1(X) + c_2 u_2(X)) &= c_1 E(u_1(X)) + c_2 E(u_2(X))
\end{align*}
\begin{align*}
    \var(X) &= \sum (x_i - \mu)^2 f(x) \\
    \var(X) &= E(X^2) - \mu^2 \\
    \var(cX) &= c^2 \var(X) \\
    \var(X + c) &= \var(X)
\end{align*}
\begin{align*}
    s^2 &= \frac{1}{n - 1} \sum_{i = 1}^n (x_i - \bar{x})^2
\end{align*}

\subsection{Bernoulli}

$p \equiv$ probability of success for each experiment.
\begin{align*}
    X &\sim \bernoulli(p) \\
    f(x) &= p^x (1-p)^{1-x} \\
    E(X) &= p \\
    \var(X) &= p(1-p)
\end{align*}

\subsection{Binomial}
$n \equiv$ number of trials, $p \equiv$ probability of success for each trial.
\begin{align*}
    X &\sim \binomial(n, p) \\
    f(x) &= \begin{pmatrix}
        n \\
        x
    \end{pmatrix} p^x (1-p)^{n-x} \\
    E(X) &= np \\
    \var(X) &= np(1-p)
\end{align*}

\subsection{Poisson}
Used when you expect to see $\lambda$ occurances over some interval. Also used to approximate a binomial distribution when $n$ is large and $p$ is small, in which case $\lambda = np$. If the interval used for $\lambda$ is not the interval of the probability to be found, replace $\lambda$ with $\lambda h$ where $h$ scales the intervals to match.
\begin{align*}
    X &\sim \poisson(\lambda) \\
    f(x) &= \frac{e^{-\lambda} \lambda^x}{x!} \\
    E(X) &= \lambda \\
    \var(X) &= \lambda
\end{align*}

\subsection{Exponential}
Models lifetimes and inter-arrival times. Given a process with a $\poisson(\lambda)$ distribution and an expected wait time to the first event $\theta = \frac{1}{\lambda}$,
\begin{align*}
    X &\sim \exponential(\theta) \\
    f(x) &= \frac{e^{-\frac{x}{\theta}}}{\theta} \\
    F(x) &= 1 - e^{-\frac{x}{\theta}} \\
    E(X) &= \theta \\
    \var(X) &= \theta^2
\end{align*}

\section{Continuous distributions}

\begin{align*}
    F(x) &= \int_{-\infty}^x f(y) \ dy \\
    E(X) &= \int_{-\infty}^\infty x \ f(x) \ dx \\
    \var(X) &= \int_{-\infty}^\infty (x - \mu)^2 \ f(x) \ dx
\end{align*}

\subsection{Uniform distribution}
\begin{align*}
    f(x) &= \frac{1}{b - a} \ ; \ x \in [a, b] \\
    F(x) &= \frac{x - a}{b - a} \ ; \ x \in [a, b] \\
    E(X) &= \frac{a + b}{2} \\
    \var(X) &= \frac{(b - a)^2}{12}
\end{align*}

\subsection{Normal distribution}
A linear function of normal random variables is normal. Note that invNorm inverts the normal CDF, it does not give back a $z$ value that bounds an area on both ends.
\begin{align*}
    X \sim \normal(\mu, \sigma^2) \implies \bar{X} \sim \normal\soft{\mu, \frac{\sigma^2}{n}}
\end{align*}
\begin{align*}
    \sum X_i \sim \normal(n\mu, n\sigma^2)
\end{align*}
\begin{align*}
    z &= \frac{X - \mu}{\sigma_X}
\end{align*}
\begin{align*}
    X_i &\sim \normal(\mu, \sigma_X^2) \\
    \bar{X} &\sim \normal\soft{\mu, \frac{\sigma_X^2}{n}} \\
    \sigma_{\bar{X}}^2 &= \frac{\sigma_X^2}{n} \\
    z &= \frac{\bar{X} - \mu}{\sigma_{\bar{X}}}
\end{align*}

\subsection{Central limit theorem}
Given $n \geq 30$ and $X \sim \text{Unknown}$,
\begin{align*}
    \bar{X} \ &\dot{\sim} \ \normal\soft{\mu, \frac{\sigma^2}{n}}
\end{align*}

\subsection{Student's $t$ distribution}
When $\sigma^2$ is unknown, use the $t$ distribution with $n - 1$ degrees of freedom.
\begin{align*}
    \frac{\bar{X} - \mu}{\soft{\frac{s}{\sqrt{n}}}} \sim t_{n - 1}
\end{align*}

\section{Interval estimation}

\subsection{Confidence intervals}
\begin{align*}
    \bar{X} &\pm \frac{\sigma}{\sqrt{n}} \times z_{\frac{\alpha}{2}} \\
    \bar{X} &\pm \frac{s}{\sqrt{n}} \times t_{\frac{\alpha}{2}, n - 1}
\end{align*}

\subsection{Confidence intervals for binomial processes}
Applies when given a population proportion $\hat{p}$. If $n\hat{p} \geq 5$ and $n(1 - \hat{p}) \geq 5$, then
\begin{align*}
    \hat{p} &\pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{p} (1 - \hat{p})}{n}}
\end{align*}

\section{Hypothesis testing}

\subsection{State hypotheses}
Include $H_0$ and $H_a$.
\begin{center}
    \begin{tabular}{|l|l|l|}
        \hline
                     & $H_0$ true            & $H_0$ false           \\ \hline
        Reject $H_0$ & \textbf{Type 1 error} & Correct               \\ \hline
        Keep $H_0$   & Correct               & \textbf{Type 2 error} \\ \hline
    \end{tabular}
\end{center}

\subsection{Test statistic}
\begin{align*}
    z &= \frac{\bar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \\
    t &= \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
\end{align*}
\begin{align*}
    \text{Right tailed:} \quad & H_0: \mu > \mu_0 \\
    \text{Left tailed:}  \quad & H_0: \mu < \mu_0 \\
    \text{Two tailed:}   \quad & H_0: \mu \neq \mu_0
\end{align*}

\subsection{p-value}
\begin{align*}
    \alpha &\equiv \text{desired significance} \\
    p &\equiv \text{attained significance} \\
    p &= P(H_a | H_0)
\end{align*}

Find the probability of getting the attained statistic if $H_0$ is true. If $p < \alpha$, meaning that it is less likely that $\alpha$ to see this value when $H_0$ is true, then reject $H_0$.

Rejection conditions:
\begin{center}
    \begin{tabular}{|l|l|l|l|}
        \hline
                         & $H_0: \mu > \mu_0$ & $H_0: \mu < \mu_0$ & $H_0: \mu \neq \mu_0$ \\ \hline
        $\sigma$ known   & $P(Z > z)$         & $P(Z < z)$         & $2P(Z > z)$           \\ \hline
        $\sigma$ unknown & $P(T > t)$         & $P(T < t)$         & $2P(T > t)$           \\ \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{|l|l|l|l|}
        \hline
                              & $\sigma$ known                   & $\sigma$ unknown                   \\ \hline
        $H_0: \mu > \mu_0$    & $\normalcdf(z, \infty)$          & $\tcdf(t, \infty, n - 1)$          \\ \hline
        $H_0: \mu < \mu_0$    & $\normalcdf(-\infty, z)$         & $\tcdf(-\infty, t, n - 1)$         \\ \hline
        $H_0: \mu \neq \mu_0$ & $2 \times \normalcdf(z, \infty)$ & $2 \times \tcdf(t, \infty, n - 1)$ \\ \hline
    \end{tabular}
\end{center}

\subsection{Critical section}

Rejection conditions:
\begin{center}
    \begin{tabular}{|l|l|l|l|}
        \hline
                         & $H_0: \mu > \mu_0$        & $H_0: \mu < \mu_0$         & $H_0: \mu \neq \mu_0$     \\ \hline
        $\sigma$ known   & $z > z_\alpha$            & $z < -z_\alpha$            & $|z| > z_\alpha$          \\ \hline
        $\sigma$ unknown & $t > t_{\alpha, n - 1}$   & $t < -t_{\alpha, n - 1}$   & $|t| > t_{\alpha, n - 1}$ \\ \hline
    \end{tabular}
\end{center}

\subsection{Conclusion statement}

\end{multicols*}
\end{document}